{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = https://project.lsst.org/sites/default/files/Rubin-O-Logo_0.png width=250 style=\"padding: 10px\"> \n",
    "<b>Introduction to the LSST data Butler</b> <br>\n",
    "Contact author(s): Alex Drlica-Wagner, Melissa Graham <br>\n",
    "Last verified to run: 2022-06-27 <br>\n",
    "LSST Science Piplines version: Weekly 2022_22 <br>\n",
    "Container Size: medium <br>\n",
    "Targeted learning level: beginner <br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on\n",
    "import logging\n",
    "logging.getLogger(\"flake8\").setLevel(logging.FATAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Learn about how to query and access data through the Butler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skills:** Discover, query, and retrieve image and catalog data with the Butler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSST Data Products:** Calexp and coadd images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Packages:** lsst.daf.butler, lsst.geom, lsst.afw.coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit:** Elements of this tutorial were originally developed by Alex Drlica-Wagner in the context of the LSST Stack Club. Please consider acknowledging Alex Drlica-Wagner in any publications or software releases that make use of this notebook's contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Support:**\n",
    "Find DP0-related documentation and resources at <a href=\"https://dp0-2.lsst.io\">dp0-1.lsst.io</a>. Questions are welcome as new topics in the <a href=\"https://community.lsst.org/c/support/dp0\">Support - Data Preview 0 Category</a> of the Rubin Community Forum. Rubin staff will respond to all questions posted there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In the introductory Butler tutorial, we learned how to access DP0 data given a specific data identifier (`dataId`). In this tutorial, we will explore how to use the Butler to find available data sets that match different sets of criteria (i.e., perform spatial and temporal searches). As a reminder, full Butler documentation can be found [here](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/index.html). For this notebook in particular, you might find this set of [Frequently Asked Questions](https://pipelines.lsst.io/middleware/faq.html) to be useful.\n",
    "\n",
    "This notebook demonstrates how to:<br>\n",
    "1. Create an instance of the Butler<br>\n",
    "2. Retrieve images using various query constraints<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Package Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import general python packages and several packages from the LSST Science Pipelines, including the Butler package and AFW Display, which will be used to display images.\n",
    "More details and techniques regarding image display with `AFW Display` can be found in the `rubin-dp0` GitHub Organization's [tutorial-notebooks](https://github.com/rubin-dp0/tutorial-notebooks) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic python packages\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import astropy.time\n",
    "from astropy.io import fits\n",
    "\n",
    "# LSST Science Pipelines (Stack) packages\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.afw.display as afwDisplay\n",
    "import lsst.geom as geom\n",
    "import lsst.sphgeom\n",
    "import lsst.afw.coord as afwCoord\n",
    "from lsst.afw.image import ExposureF\n",
    "afwDisplay.setDefaultBackend('matplotlib')\n",
    "\n",
    "# Set a standard figure size to use\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an instance of the Butler\n",
    "\n",
    "First, we create an instance of the Butler pointing to the DP0.2 data. We do this by specifying the `dp02` configuration and the `2.2i/runs/DP0.2` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Butler pointing to the DP0.2 repository\n",
    "config = 'dp02'\n",
    "collections = '2.2i/runs/DP0.2'\n",
    "butler = dafButler.Butler(config=config, collections=collections)\n",
    "registry = butler.registry\n",
    "\n",
    "# Note: This will trigger a warning from CFITSIO in w_2022_22.\n",
    "# This warning can be safely ignored and will be corrected in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on the Butler can be found through the help string of our Butler instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Explore the DP0 data repository\n",
    "\n",
    "Butler repositories have both a database component and a file-like storage component. The database component can be accessed through the Butler registry, while file-like storage can be local (i.e., pointing to a directory on the local file system) or remote (i.e., pointing to cloud storage resources).\n",
    "DP0 uses Simple Storage Service (S3) buckets, which are public cloud storage resources that are similar to file folders, store objects, and which consist of data and its descriptive metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database side of a data repository is called a `registry`.\n",
    "The registry contains entries for all data products, and organizes them by _collections_, _dataset types_, and _data IDs_.\n",
    "We can access a registry client directly as part of our Butler object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn more about the registry by uncommenting the following line.\n",
    "# help(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collections are lightweight groups of datasets such as the set raw images for a particular instrument, self-consistent calibration datasets, and the outputs of a processing run.\n",
    "For DP0.2, we use the `2.2i/runs/DP0.2` collection, which we specified when creating our instance of the butler.\n",
    "However, it is possible to access other collections, which can be queried with `queryCollections`. \n",
    "We won't got through these other collections, but you can find out more about collections in the [documentation](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/organizing.html#collections) and on the [FAQ](https://pipelines.lsst.io/middleware/faq.html#querycollections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Butler data IDs\n",
    "\n",
    "The data ID is a dictionary-like identifier for a data product (more information can be found [here](https://pipelines.lsst.io/modules/lsst.daf.butler/dimensions.html#data-ids)).\n",
    "Each `DatasetType` (i.e., `calexp`, `deepCoadd`, `objectTable`, etc.) uses a different set of keys in its data ID, which are also called \"dimensions\". \n",
    "We can use the registry to access a specific named dataset type and list its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the registry to access the calexp datasetType.\n",
    "# queryDatasetTypes returns a generator, so we need to call `next` to get the first entry.\n",
    "dt = next(registry.queryDatasetTypes('calexp'))\n",
    "print(\"Name:\",dt.name)\n",
    "print(\"Dimensions:\", dt.dimensions)\n",
    "print(\"Storage Class:\", dt.storageClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The data ID contains both *implied* and *required* keys.\n",
    "For example, the value of `band` is *implied* by the `visit`, because a single visit refers to a single exposure at a single pointing in a single band.\n",
    "\n",
    "In other tutorial notebooks, we have seen how to access a specific data product using a fully specified data ID. A query for a fully specified data ID should return one unique entry (however, see the [FAQ](https://pipelines.lsst.io/middleware/faq.html#why-do-queries-return-duplicate-results) entry about duplicate results from chained collections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetType = 'calexp'\n",
    "dataId = {'visit': 192350, 'detector': 175}\n",
    "datasetRefs = registry.queryDatasets(datasetType, dataId=dataId)\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    print(ref.dataId)\n",
    "    print(ref.dataId.full)\n",
    "    print(\"band:\",ref.dataId['band'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data IDs can be represented as regular Python `dict` objects, but when they are returned from the `Butler` the `DataCoordinate` class is used instead.\n",
    "Printing the data ID shows only the required keys, but all keys can be shown by specifying `.full` member (see the [FAQ](https://pipelines.lsst.io/middleware/faq.html#why-are-some-keys-usually-filters-sometimes-missing-from-data-ids)).\n",
    "The value of a single key, in this case *band*, can also be printed by specifying the key name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to query for all data products that match a partially specified data ID.\n",
    "For example, in the following cell we use a partially specified data ID to select all the `calexp` data associated with visit=192350.\n",
    "This search will return a separate entry for each CCD detector that was processed for this visit. \n",
    "We'll print information about a few of them.\n",
    "\n",
    "(The following cell will fail and return an error if the query is requesting a `DatasetRef` for data that does not exist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partially specified dataId\n",
    "datasetType = 'calexp'\n",
    "dataId = {'visit': 192350}\n",
    "datasetRefs = set(registry.queryDatasets(datasetType, dataId=dataId))\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    print(ref.dataId.full)\n",
    "    if i > 5:\n",
    "        print('...')\n",
    "        break\n",
    "        \n",
    "print(f\"Found {len(list(datasetRefs))} detectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of the `set` method in the previous cell. \n",
    "If you remove the `set` command, you will find that `queryDatasets` returns duplicate entries for some detectors. \n",
    "This is the result of a conscious design choice in order to to handle large results efficiently. In essence, the SQL queries that are being executed \"under the hood\" are not attempting to remove duplicate entries. You can find a more extensive discussion of duplicate query entries in the the Middleware [FAQ](https://pipelines.lsst.io/middleware/faq.html#why-do-queries-return-duplicate-results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Finding Resources\n",
    "\n",
    "\n",
    "One of the beauties of the butler is that there is no need to know exactly where the data live in order to access it. Passing a datasetRef to Butler.get will return an instance of the appropriate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ref in enumerate(datasetRefs):\n",
    "    calexp = butler.get(ref)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasetRef also provides access to a Uniform Resource Identifier (URI) for each data product.\n",
    "The URI is the closest thing to a \"filepath to the data\". \n",
    "However, for DP0 this URI does not refer to a local path on the filesystem.\n",
    "Instead, it points to an [cloud storage bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-buckets-s3.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ref in enumerate(datasetRefs):\n",
    "    uri = butler.getURI(ref)\n",
    "    print('File URI: ', uri)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bucket does not exist on your local filesystem, so you cannot access it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the URI is actually an object of type `S3ResourcePath`. This allows us to learn various things about the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the folowing line to get help on the URI\n",
    "#help(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filesize:\",uri.size())\n",
    "print(\"Network Location:\", uri.netloc)\n",
    "print(\"Path:\", uri.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the contents of the bucket locally. The `as_local` method will return a context manager that can be used to create a temporary file with a copy of the information contained in the bucket. This temporary file will be deleted afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with uri.as_local() as local:\n",
    "    path = local.ospath\n",
    "    !ls {path}\n",
    "    hdulist = fits.open(path)\n",
    "print(\"We loaded a:\", type(hdulist))\n",
    "!ls {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that this bucket contains a `calexp`, we can also instantiate an ExposureF object with the temporary file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with uri.as_local() as local:\n",
    "    path = local.ospath\n",
    "    calexp = ExposureF(path)\n",
    "print(\"We loaded a:\", type(calexp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Querying data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Basic Queries\n",
    "\n",
    "Our example above demonstrated a very simple use of `queryDatasets`, but additional query terms can also be used, such as band and visit.\n",
    "When a query term is an equality, it can be specified as an argument like `band=''`. \n",
    "When a query term is an inequality, it can be specified with `where`.\n",
    "More details on Butler queries can be found [here](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/queries.html).\n",
    "\n",
    "In the following cell, we select all calexps corresponding i-band observations of a single detector over a range of visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRefs = registry.queryDatasets(datasetType='calexp', band='i', detector=175,\n",
    "                                     where='visit > 192000 and visit < 193000'\n",
    "                                     )\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    print(ref.dataId.full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexp.visitInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.2 Temporal queries\n",
    "\n",
    "The following examples show how to query for data sets that include a desired coordinate and observation date.\n",
    "\n",
    "Let's start by accessing the calexp that we accessed earlier in the notebook. We do this by passing the fully specified dataId to the butler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId = {'visit': 971990, 'detector': 175}\n",
    "#dataId = {'visit': 192350, 'detector': 175}\n",
    "calexp = butler.get('calexp',dataId=dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out information about the visit that this calexp was derived from, we can use the visitInfo member.\n",
    "In particular, we can access the RA,Dec of the telescope boresight and the observation date.\n",
    "These are just human-readable summaries of the more precise spatial and temporal information stored in the registry, which are represented in Python by `Timespan` and `Region` objects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calexp.visitInfo)\n",
    "print(\"Telescope Boresight:\",calexp.visitInfo.boresightRaDec)\n",
    "print(\"Observation DateTime:\",calexp.visitInfo.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the region of sky that is covered by this calexp by getting the convex polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexp.getConvexPolygon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we query for `deepCoadd` datasets with a `visit`+`detector` data ID, we'll get just the deepCoadd objects that overlap that observation and have the same band (because a visit implies a band):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in registry.queryDatasets(\"deepCoadd\", dataId=dataId):\n",
    "    print(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query for dimension records or datasets that overlap an arbitrary time range, we can use the `bind` argument to pass times through to `where`.\n",
    "Using `bind` to define an alias for a variable saves us from having to string-format the times into the `where` expression.\n",
    "Note that a `dafButler.Timespan` will accept a `begin` or `end` value that is equal to `None` if it is unbounded on that side.\n",
    "\n",
    "Use `bind` and `where`, along with [astropy.time](https://docs.astropy.org/en/stable/time/index.html), to look for visits within +/- 90 seconds of this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = astropy.time.Time(calexp.visitInfo.date.toPython())\n",
    "minute = astropy.time.TimeDelta(60, format=\"sec\")\n",
    "timespan = dafButler.Timespan(time - 10*minute, time + 10*minute)\n",
    "\n",
    "datasetRefs = registry.queryDatasets(\"calexp\",\n",
    "                                     where=\"visit.timespan OVERLAPS my_timespan\",\n",
    "                                     bind={\"my_timespan\": timespan})\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    print(ref)\n",
    "    if i > 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Spatial queries\n",
    "\n",
    "Arbitrary spatial queries are not supported at this time, such as the \"POINT() IN (REGION)\" example found in this [Butler queries](https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/queries.html) documentation.\n",
    "In other words, at this time it is only possible to do queries involving regions that are already \"in\" the data repository, either because they are HTM pixel regions or because they are tract/patch/visit/visit+detector regions.\n",
    "\n",
    "Thus, for this example we use the set of dimensions that correspond to different levels of the HTM (hierarchical triangular mesh) pixelization of the sky ([HTM primer](http://www.skyserver.org/htm/)).\n",
    "The process is to transform a region or point into one or more HTM identifiers (HTM IDs), and then create a query using the HTM ID as the spatial data ID.\n",
    "The `lsst.sphgeom` library supports region objects and HTM pixelization in the LSST Science Pipelines.\n",
    "\n",
    "Import the `lsst.sphgeom` package, initialize a sky pixelization to level 10 (the level at which one sky pixel is about five arcmin across), and find the HTM ID for a desired sky coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelization = lsst.sphgeom.HtmPixelization(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra,dec = calexp.visitInfo.boresightRaDec\n",
    "htm_id = pixelization.index(\n",
    "    lsst.sphgeom.UnitVector3d(\n",
    "        lsst.sphgeom.LonLat.fromDegrees(ra.asDegrees(), dec.asDegrees())\n",
    "    )\n",
    ")\n",
    "\n",
    "# Obtain and print the scale to provide a sense of the size of the\n",
    "# sky pixelization being used\n",
    "circle = pixelization.triangle(htm_id).getBoundingCircle()\n",
    "scale = circle.getOpeningAngle().asDegrees()*3600.\n",
    "level = pixelization.getLevel()\n",
    "print(f'HTM ID={htm_id} at level={level} is a ~{scale:0.2} arcsec triangle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRefs = registry.queryDatasets(\"calexp\", htm20=htm_id,\n",
    "                                     where=\"visit.timespan OVERLAPS my_timespan\",\n",
    "                                     bind={\"my_timespan\": timespan})\n",
    "\n",
    "for i, ref in enumerate(datasetRefs):\n",
    "    print(ref)\n",
    "    if i > 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, with the above query, we have uniquely identified the visit and detector for our desired temporal and spatial constraints.\n",
    "\n",
    "Note that if a smaller HTM level is used (like 7), which is a larger sky pixel (~2200 arcseconds), the above query will return many more visits and detectors which overlap with that larger region. Try it and see!\n",
    "\n",
    "Note that queries using the HTM ID can also be used to, e.g., find the set of all i-band `src` catalog data products that overlap this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, src_ref in enumerate(registry.queryDatasets(\"src\", htm20=htm_id,\n",
    "                                                   band=\"i\")):\n",
    "    print(src_ref)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is does that search take tens of seconds?\n",
    "The butler's spatial reasoning is designed to work well for regions the size of full data products, like detector- or patch-level images and catalogs, and it's a poor choice for object-scale searches.\n",
    "The above search is slow in part because `queryDatasets` searches for all `src` datasets that overlap a larger region and then filters the results down to the specified HTM ID pixel.\n",
    "\n",
    "For searches of these scales, it is often more efficient to use the TAP service, as demonstrated in later tutorials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
